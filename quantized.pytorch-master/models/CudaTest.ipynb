{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f84117437594>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xuxiayu\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xuxiayu\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xuxiayu\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[1;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[1;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m                 \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xuxiayu\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xuxiayu\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m    161\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optional: Data Parallelism\n",
    "==========================\n",
    "**Authors**: `Sung Kim <https://github.com/hunkim>`_ and `Jenny Kang <https://github.com/jennykang>`_\n",
    "\n",
    "In this tutorial, we will learn how to use multiple GPUs using ``DataParallel``.\n",
    "\n",
    "It's very easy to use GPUs with PyTorch. You can put the model on a GPU:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)\n",
    "\n",
    "Then, you can copy all your tensors to the GPU:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    mytensor = my_tensor.to(device)\n",
    "\n",
    "Please note that just calling ``my_tensor.to(device)`` returns a new copy of\n",
    "``my_tensor`` on GPU instead of rewriting ``my_tensor``. You need to assign it to\n",
    "a new tensor and use that tensor on the GPU.\n",
    "\n",
    "It's natural to execute your forward, backward propagations on multiple GPUs.\n",
    "However, Pytorch will only use one GPU by default. You can easily run your\n",
    "operations on multiple GPUs by making your model run parallelly using\n",
    "``DataParallel``:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "That's the core behind this tutorial. We will explore it in more detail below.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Imports and parameters\n",
    "# ----------------------\n",
    "#\n",
    "# Import PyTorch modules and define parameters.\n",
    "#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Parameters and DataLoaders\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 30\n",
    "data_size = 100\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Device\n",
    "#\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "######################################################################\n",
    "# Dummy DataSet\n",
    "# -------------\n",
    "#\n",
    "# Make a dummy (random) dataset. You just need to implement the\n",
    "# getitem\n",
    "#\n",
    "\n",
    "class RandomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Simple Model\n",
    "# ------------\n",
    "#\n",
    "# For the demo, our model just gets an input, performs a linear operation, and\n",
    "# gives an output. However, you can use ``DataParallel`` on any model (CNN, RNN,\n",
    "# Capsule Net etc.)\n",
    "#\n",
    "# We've placed a print statement inside the model to monitor the size of input\n",
    "# and output tensors.\n",
    "# Please pay attention to what is printed at batch rank 0.\n",
    "#\n",
    "\n",
    "class Model(nn.Module):\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.fc(input)\n",
    "        print(\"\\tIn Model: input size\", input.size(),\n",
    "              \"output size\", output.size())\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Create Model and DataParallel\n",
    "# -----------------------------\n",
    "#\n",
    "# This is the core part of the tutorial. First, we need to make a model instance\n",
    "# and check if we have multiple GPUs. If we have multiple GPUs, we can wrap\n",
    "# our model using ``nn.DataParallel``. Then we can put our model on GPUs by\n",
    "# ``model.to(device)``\n",
    "#\n",
    "\n",
    "model = Model(input_size, output_size)\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Run the Model\n",
    "# -------------\n",
    "#\n",
    "# Now we can see the sizes of input and output tensors.\n",
    "#\n",
    "\n",
    "for data in rand_loader:\n",
    "    input = data.to(device)\n",
    "    output = model(input)\n",
    "    print(\"Outside: input size\", input.size(),\n",
    "          \"output_size\", output.size())\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Results\n",
    "# -------\n",
    "#\n",
    "# If you have no GPU or one GPU, when we batch 30 inputs and 30 outputs, the model gets 30 and outputs 30 as\n",
    "# expected. But if you have multiple GPUs, then you can get results like this.\n",
    "#\n",
    "# 2 GPUs\n",
    "# ~~~~~~\n",
    "#\n",
    "# If you have 2, you will see:\n",
    "#\n",
    "# .. code:: bash\n",
    "#\n",
    "#     # on 2 GPUs\n",
    "#     Let's use 2 GPUs!\n",
    "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#         In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "#         In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "#\n",
    "# 3 GPUs\n",
    "# ~~~~~~\n",
    "#\n",
    "# If you have 3 GPUs, you will see:\n",
    "#\n",
    "# .. code:: bash\n",
    "#\n",
    "#     Let's use 3 GPUs!\n",
    "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "#\n",
    "# 8 GPUs\n",
    "# ~~~~~~~~~~~~~~\n",
    "#\n",
    "# If you have 8, you will see:\n",
    "#\n",
    "# .. code:: bash\n",
    "#\n",
    "#     Let's use 8 GPUs!\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Summary\n",
    "# -------\n",
    "#\n",
    "# DataParallel splits your data automatically and sends job orders to multiple\n",
    "# models on several GPUs. After each model finishes their job, DataParallel\n",
    "# collects and merges the results before returning it to you.\n",
    "#\n",
    "# For more information, please check out\n",
    "# https://pytorch.org/tutorials/beginner/former\\_torchies/parallelism\\_tutorial.html.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
